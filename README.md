高通AI研究通过全栈AI优化在边缘设备上部署流行的1B+参数基础模型
基础模型正在席卷人工智能 （AI） 行业。基础模型是一个大型神经网络，可以在大量数据上进行大规模训练，从而产生一个可以适应各种下游任务且高性能的模型。稳定扩散是一种非常流行的基础模型，是一种文本到图像生成 AI 模型，能够在数十秒内根据任何文本输入创建逼真的图像——非常不可思议。到目前为止，在超过1亿个参数下，稳定扩散主要局限于在云中运行。继续阅读以了解高通AI Research如何使用Qualcomm AI Stack执行全栈AI优化，以首次在Android智能手机上部署稳定扩散。


全栈AI优化，可在设备上完全高效运行稳定扩散。

使用高通AI堆栈进行全栈AI优化
在我的“AI firsts”博客文章中，我描述了高通人工智能研究公司如何不仅承担起产生新颖的人工智能研究的任务，而且还率先在商业设备上展示概念验证——为技术在现实世界中的扩展铺平了道路。我们的全栈人工智能研究意味着跨应用程序、神经网络模型、算法、软件和硬件进行优化，以及跨公司内部的跨学科工作。对于稳定扩散，我们从拥抱脸的 FP32 版本 1-5 开源模型开始，并通过量化、编译和硬件加速进行了优化，以便在搭载骁龙 8 Gen 2 移动平台的手机上运行。

为了将模型从FP32缩小到INT8，我们使用了AI模型效率工具包（AIMET）的训练后量化，该工具由高通AI Research创建的技术开发而成，现已纳入新发布的Qualcomm AI Studio。量化不仅可以提高性能，还可以通过允许模型在我们的专用 AI 硬件上高效运行并消耗更少的内存带宽来节省功耗。我们最先进的AIMET量化技术，如自适应舍入（AdaRound），能够在这种较低的精度下保持模型精度，而无需重新训练。这些技术应用于稳定扩散中的所有组件模型，即基于变压器的文本编码器、VAE解码器和UNet。这对于模型适合设备至关重要。

为了进行编译，我们使用高通AI引擎直接框架将神经网络映射到一个在我们的目标硬件上高效运行的程序。高通AI引擎直接框架根据高通海克斯康处理器的硬件架构和内存层次结构对操作进行排序，以提高性能并最大限度地减少内存溢出。其中一些增强功能源于 AI 优化研究人员与编译器工程团队合作，以改进 AI 推理中的内存管理。高通AI引擎中所做的整体优化显着降低了运行时延迟和功耗，并且这种急需的趋势在稳定扩散中仍在继续。

我们在采用海克斯康处理器的高通AI引擎上具有行业领先的AI性能，通过紧密的硬件和软件协同设计得以释放。最新的骁龙 8 Gen 2 具有微磁贴推理功能，有助于使稳定扩散等大型模型高效运行——预计下一代骁龙会有更多改进。此外，为变压器模型（如MobileBERT）所做的增强功能可显著加快推理速度，这在稳定扩散的整个组件模型中都使用了多头注意力，因此在这里起着关键作用。

这种全栈优化的结果是在智能手机上运行稳定扩散，不到 15 秒，进行 20 个推理步骤，生成 512×512 像素的图像——这是智能手机上最快的推理，可与云延迟相媲美。用户文本输入完全不受约束。


交流群：![本地路径](微信图片_20231026134238.jpg "加群") <!-- 此路径表示图片和MD文件，处于同一目录 -->

VX:ppoluo
